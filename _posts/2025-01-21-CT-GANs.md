## Conditional-Table-GAN

---

[//]: # (https://proceedings.neurips.cc/paper_files/paper/2019/file/254ed7d2de3b23ab10936522dd547b78-Paper.pdf)
![alt text](https://github.com/StefanoPenazzi2/StefanoPenazzi2.github.io/blob/main/imgs/ctgans/ctgans_architecture.png?raw=true)

**Synthetic Tabula Data Generation** 

Synthetic data generation has involved treating each column in a table as a random variable, modeling a joint multivariate
probability distribution, and sampling from it. Table below shows a few synthetic data generation methods.

<style scoped>
table {
  font-size: 11px;
}
</style>

| Method                     | Pros                                                                                   | Cons                                                                                       |
|----------------------------|----------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------|
| Decision Trees             | - Easy to interpret and implement. <br> - Handles both categorical and continuous data. | - Prone to overfitting. <br> - Limited in modeling interactions between variables.          |
| Bayesian Networks          | - Efficiently models conditional dependencies. <br> - Provides insights into causal relationships. | - Computationally intensive for large datasets. <br> - Requires domain expertise.         |
| Spatial Decomposition Trees | - Well-suited for spatial data. <br> - Captures spatial dependencies effectively.      | - Complexity increases with dimensionality. <br> - Requires domain-specific tuning.        |
| Copulas                    | - Models non-linear dependencies. <br> - Separate modeling of margins and dependencies. | - Difficult to specify copula types. <br> - Computationally demanding for high-dimensional data. |
| Markov Chain Monte Carlo (MCMC) | - Flexible and applicable to various distributions. <br> - Strong theoretical foundation.   | - Computationally expensive. <br> - Requires careful tuning.                               |
| Bootstrapping              | - Simple and non-parametric. <br> - Useful for small samples.                          | - May not capture data complexity. <br> - Produces highly similar samples.                |

Over the last decade generative methods like GANs, VAEs, DDPMs and their variaces have been used to generate
accurate synthetic data. Below a list of resons:

| Reason                                  | Description                                                                                                  |
|-----------------------------------------|--------------------------------------------------------------------------------------------------------------|
| High-Dimensional Learning               | Can capture intricate patterns in high-dimensional data.                                                     |
| Flexibility                             | Learns non-linear relationships for robust data generation.                                                  |
| Rich Feature Representations            | Learns hierarchical features for detailed and realistic data generation.                                     |
| Scalability                             | Scales well with large datasets for consistent quality at scale.                                             |

Original GAN architectures and loss functions may not be well-suited to addressing the challenges associated with tabular data.
Below is a list of these challenges:

| Challenge                                    | Description                                                                                                                                                                                             |
|----------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Mixed Data Types                             | Real-world tabular data consists of mixed types. GANs need to use both softmax and tanh to generate a mix of discrete and continuous columns.                                                           |
| Non-Gaussian Distributions                   | Continuous values in tabular data are usually non-Gaussian, leading to vanishing gradient problems with min-max transformations.                                                                        |
| Multimodal Distributions                     | Many continuous columns have multiple modes. Vanilla GANs struggle to model all modes, particularly in multimodal distributions.                                                                        |
| Learning from Sparse One-Hot Encoded Vectors | GANs generate probability distributions over categories, while real data is in one-hot vectors. Discriminators can easily detect fake data by the sparseness of the distribution rather than the row's realness. |
| Highly Imbalanced Categorical Columns        | Many categorical columns are highly imbalanced, causing mode collapse and inadequate training for minor classes. Missing minor categories leads to subtle distribution changes unnoticed by the discriminator. |


**Why standard GANs are not suitable for tabular data**

***Continuous Features***

Min-max normalization to normalize continuous values to $[âˆ’1, 1]$ cannot deal with complicated distributions.
The mode-specific normalization method contains three steps:

- For each continuous column $C_i$:
   - Use variational Gaussian mixture model (VGMM) to estimate the number of modes $m_i$ and fit a Gaussian mixture:
        $$
        P_{C_i}(c_{i,j}) = \sum_{k=1}^{n} \mu_k N(c_{i,j}; \eta_k, \phi_k)
        $$

   - For each value $c_{i,j}$ in $C_i$:
     1. Calculate mode probabilities:
        $$
        \rho_{k,i,j} = \mu_k N(c_{i,j}; \eta_k, \phi_k)
        $$
     2. Sample a mode from $\rho_{k,i,j}$. 
     3. Represented $c_{i,j}$ as:
        - $\beta_{i,j} = [0,0,1]$ (One-hot for sampled mode)
        - $\alpha_{i,j} = \frac{c_{i,j} - \eta_3}{4\phi_3}$ (Normalized value within sampled mode)

- Concatenate to form row representation ($d_{i,j}$ is one-hot representation of a discrete value):
  $$
  \mathbf{x}_j = \alpha_{1,j} \oplus \beta_{1,j} \oplus \dots \oplus \alpha_{N_c,j} \oplus \beta_{N_c,j} \oplus \mathbf{d}_{1,j} \oplus \dots \oplus \mathbf{d}_{N_d,j}
  $$

***Categorical Features***

Class imbalance in categorical columns $D$ poses challenges for standard GAN generator training.
Typically, GANs sample from a Multivariate Normal Distribution (MVN), which does not account for class imbalance,
leading to underrepresentation of minor categories.
Resampling techniques, such as oversampling or undersampling, can adjust for this imbalance, but they result in
the generator learning a distribution different from the original data.

The objective is to resample efficiently so that all categories $d_{i,j} of discrete attributes
are sampled evenly (though not necessarily uniformly) during training, while recovering the
true data distribution during testing.
The generator is trained to approximate conditional distributions based on known categorical variables:

$$
X \mid \hat{d_{i,j}} \sim Q_\theta 
$$

![alt text](https://github.com/StefanoPenazzi2/StefanoPenazzi2.github.io/blob/main/imgs/ctgans/ctgans_conditional_input.png?raw=true)

Using the law of total probability, we can express the row probability as:

$$
\mathbf{P}(r) = \sum_{j \in d_i} Q_\theta(X=x) \mathbf{P}(d_{i,j}) 
$$

This approach ensures that the generator maintains fidelity to the original data distribution while effectively addressing class imbalance.

This approach requires three things:

- 1. Prepare a conditional input for the generator and the discriminator
- 2. Ensure that the generator preserves the input conditions
- 3. Ensure the conditional generator accurately learns the true conditional distribution of the data



***Conditional Vector***

Let $m_{i^*}$ be the mask vector associated with the one-hot vector representation of $D_i$

$$
m_{i,j} =
\begin{cases} 
1, & \text{if } i = i^* \text{ and } j = j^*, \\
0, & \text{otherwise}.
\end{cases}
$$

Then, define the vector

$$
\mathbf{c} = m_{1} \oplus \ldots \oplus m_{N_d}. 
$$

***Generator Loss***

![alt text](https://github.com/StefanoPenazzi2/StefanoPenazzi2.github.io/blob/main/imgs/ctgans/ctgans_generatorloss.png?raw=true)

During training, the conditional generator is free to produce any set of one-hot discrete vectors.
The proposed mechanism to ensure the conditional generator produces 

$$
\hat{d}_{i^*} = m_{i^*} 
$$ 

is to penalize its loss by adding the cross-entropy between $$ m_{i^*} $$ and $$ \hat{d}_{i^*} $$ averaged over all instances in the batch.

$$
\frac{1}{M}H(m_{i^*}, \hat{d_{i^*}})
$$

***Training-by-sampling***

![alt text](https://github.com/StefanoPenazzi2/StefanoPenazzi2.github.io/blob/main/imgs/ctgans/ctgans_trainingbysampling.png?raw=true)


$$ 
\min_{\theta} \max_{\omega} F(\theta, \omega) = \min_{\theta} \max_{\omega} \left( \mathbb{E}_{x \sim P} \left[g_f(V_\omega(x))\right] + \mathbb{E}_{x \sim Q_\theta} \left[\log(-g_f(V_\omega(x)))\right] + 1 + \lambda \cdot \frac{1}{|M|} \sum_{i \in \textit(M)}H(m_{i}, \hat{d_{i}}) \right) 
$$


[//]: # ($$ )
[//]: # (F&#40;\theta, \omega&#41; = \mathbb{E}_{x \sim P} \left[g_f&#40;V_\omega&#40;x | m_i^k&#41;&#41;\right] + \mathbb{E}_{x \sim Q_\theta} \left[\log&#40;-g_f&#40;V_\omega&#40;x | m_i^k&#41;&#41;&#41;\right] + 1 + \lambda \cdot H&#40;m_i^k, \hat{m_i^k}&#41; )
[//]: # ($$)
