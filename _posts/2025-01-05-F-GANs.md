## F-divergence Generative Adversarial Neural Networks

---

### Convex conjugate

**Convex conjugate** _Let $ f : I \to \mathbb{R} $ be a convex function, where $ I \subseteq \mathbb{R} $ is an interval. The convex conjugate of $ f $ is another function $ f^* : I^* \to \mathbb{R} $ defined as:_

$$
f^*(y) = \sup_{x \in \mathbb{R}^n} \left\{ \langle y, x \rangle - f(x) \right\},
$$

_where $ I^* $ is the domain of $ f^* $, determined by the values of $ y $ for which the supremum is finite._ 

![alt text](https://github.com/StefanoPenazzi2/StefanoPenazzi2.github.io/blob/main/imgs/convex_conjugate.png?raw=true)

### F-divergences

![alt text](https://github.com/StefanoPenazzi2/StefanoPenazzi2.github.io/blob/main/imgs/2d_dist_comp_gans.png?raw=true)

The **\( f \)-divergence** is a measure of the difference between two probability distributions \( P \) and \( Q \) over a measurable space \( (\Omega, \mathcal{F}) \). It is defined as:

$$
D_f(P \| Q) = 
\begin{cases} 
\int_\Omega f\left(\frac{dP}{dQ}(x)\right) \, dQ(x), & \text{if } P \ll Q, \\ 
+\infty, & \text{otherwise},
\end{cases}
$$

where:

- \( \frac{dP}{dQ} \) is the **Radonâ€“Nikodym derivative** of \( P \) with respect to \( Q \),
- \( P \ll Q \) indicates that \( P \) is absolutely continuous with respect to \( Q \) (i.e., \( P(A) = 0 \) whenever \( Q(A) = 0 \)),
- \( f : \mathbb{R} \to \mathbb{R} \) is a convex function satisfying \( f(1) = 0 \).

### F-divergences duality

**Theorem 1** _For any f-divergence, we have:_

$$
D_f(P \| Q) = \sup_{g : \Omega \to \mathbb{R}} \left\{ \mathbb{E}_P[g(X)] - \mathbb{E}_Q[f^*(g(X))] \right\},
$$

where:

- $ P $ and $ Q $ are probability distributions over a common measurable space $ (\Omega, \mathcal{F}) $,
- $ f : I \to \mathbb{R} $ is a convex function,
- $ f^* $ is the convex conjugate of $ f $,
- $ \mathbb{E}_Q[f^{*}(g(X))] $ is the expectation under $ Q $, 
- $ \mathbb{E}_P[g(X)] $ is the expectation of $ g(X) $ under $ P $





$$
\begin{align}
D_f(P \| Q) = \int_\chi  f\left( \frac{dP}{dQ} (x) \right) dQ(x) \\
= \int_\chi \sup_{y \in dom(f^*)} \left( y\frac{dP}{dQ} (x) - f^*(y) \right) dQ(x) \\
\ge \int_\chi \left( g(x)\frac{dP}{dQ} (x) - f^*(g(x)) \right) dQ(x) \\
= \int_\chi g(x)\frac{dP}{dQ} (x) dQ(x) - \int_\chi f^*(g(x)) dQ(x) \\
= \int_\chi g(x) dP(x) - \int_\chi f^*(g(x)) dQ(x) \\
= \mathbb{E}_P[g(X)] - \mathbb{E}_Q[f^*(g(X))]
\end{align}
$$

![alt text](https://github.com/StefanoPenazzi2/StefanoPenazzi2.github.io/blob/main/imgs/1d_dist_comp_gans.png?raw=true)

kl_div p-q: 0.44179608357705596
kl_div q-p: 1.279359907124275

![alt text](https://github.com/StefanoPenazzi2/StefanoPenazzi2.github.io/blob/main/imgs/g_x_linear_opt.png?raw=true)

fun = -1.5290712338367665
x =  1.567, -1.039

![alt text](https://github.com/StefanoPenazzi2/StefanoPenazzi2.github.io/blob/main/imgs/g_x_poli3_opt.png?raw=true)

fun: -3.040389831703985 
x: -1.444e-01,  8.323e-01,  4.859e-01, -2.371e-01

![alt text](https://github.com/StefanoPenazzi2/StefanoPenazzi2.github.io/blob/main/imgs/g_x_deriv_f_x_opt.png?raw=true)


 fun: -1.4346356983844244
 x: 1.016e+00  9.584e-01  2.378e+00  2.332e+00