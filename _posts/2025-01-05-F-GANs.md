## F-divergence Generative Adversarial Neural Networks

---

### Convex conjugate

**Convex conjugate** _Let $ f : I \to \mathbb{R} $ be a convex function, where $ I \subseteq \mathbb{R} $ is an interval. The convex conjugate of $ f $ is another function $ f^* : I^* \to \mathbb{R} $ defined as:_

$$
f^*(y) = \sup_{x \in \mathbb{R}^n} \left\{ \langle y, x \rangle - f(x) \right\},
$$

_where $ I^* $ is the domain of $ f^* $, determined by the values of $ y $ for which the supremum is finite._ 

![alt text](https://github.com/StefanoPenazzi2/StefanoPenazzi2.github.io/blob/main/imgs/convex_conjugate.png?raw=true)


### F-divergences duality

**Theorem 1** _For any f-divergence, we have:_

$$
D_f(P \| Q) = \sup_{g : \Omega \to \mathbb{R}} \left\{ \mathbb{E}_P[g(X)] - \mathbb{E}_Q[f^*(g(X))] \right\},
$$

where:

- $ P $ and $ Q $ are probability distributions over a common measurable space $ (\Omega, \mathcal{F}) $,
- $ f : I \to \mathbb{R} $ is a convex function,
- $ f^* $ is the convex conjugate of $ f $,
- $ \mathbb{E}_Q[f^*(g(X))] $ is the expectation of $ f^*(g(X)) $ under $ P $, 
- $ \mathbb{E}_P[g(X)] $ is the expectation of $ g(X) $ under $ P $


![alt text](https://github.com/StefanoPenazzi2/StefanoPenazzi2.github.io/blob/main/imgs/1d_dist_comp_gans.png?raw=true)

kl_div p-q: 0.44179608357705596
kl_div q-p: 1.279359907124275



1. **Definition of $ f $-Divergence**:
   From the definition, we have:
   $$
   D_f(P \| Q) = \mathbb{E}_Q \left[ f\left( \frac{dP}{dQ} \right) \right].
   $$

2. **Convex Conjugate and Fenchel’s Inequality**:
   By the definition of the convex conjugate $ f^* $, for any $ u \in \mathbb{R} $ and $ v \in \mathbb{R} $, we have:
   $$
   f(u) \geq uv - f^*(v),
   $$
   with equality when $ v $ satisfies $ f^'(u) = v $.

3. **Applying to $ f $-Divergence**:
   Substituting $ u = \frac{dP}{dQ} $ and $ v = g(X) $, we get:
   $$
   f\left( \frac{dP}{dQ} \right) \geq g(X) \cdot \frac{dP}{dQ} - f^*(g(X)).
   $$

   Taking the expectation with respect to $ Q $, we have:
   $$
   \mathbb{E}_Q \left[ f\left( \frac{dP}{dQ} \right) \right] \geq \mathbb{E}_Q \left[ g(X) \cdot \frac{dP}{dQ} \right] - \mathbb{E}_Q \left[ f^*(g(X)) \right].
   $$

4. **Change of Measure**:
   Using the change of measure $ \mathbb{E}_Q \left[ g(X) \cdot \frac{dP}{dQ} \right] = \mathbb{E}_P[g(X)] $, we rewrite the inequality as:
   $$
   D_f(P \| Q) = \mathbb{E}_Q \left[ f\left( \frac{dP}{dQ} \right) \right] \geq \mathbb{E}_P[g(X)] - \mathbb{E}_Q[f^*(g(X))].
   $$

5. **Supremum Over $ g $**:
   Since the inequality holds for all measurable functions $ g : \Omega \to \mathbb{R} $, we take the supremum over $ g $ to obtain:
   $$
   D_f(P \| Q) \geq \sup_{g : \Omega \to \mathbb{R}} \left\{ \mathbb{E}_P[g(X)] - \mathbb{E}_Q[f^*(g(X))] \right\}.
   $$

6. **Tightness of the Bound**:
   The equality is achieved when $ g(X) = f'\left( \frac{dP}{dQ} \right) $, as this satisfies the equality condition in Fenchel’s inequality. Substituting $ g(X) = f'\left( \frac{dP}{dQ} \right) $, we verify:
   $$
   D_f(P \| Q) = \sup_{g : \Omega \to \mathbb{R}} \left\{ \mathbb{E}_P[g(X)] - \mathbb{E}_Q[f^*(g(X))] \right\}.
   $$

![alt text](https://github.com/StefanoPenazzi2/StefanoPenazzi2.github.io/blob/main/imgs/2d_dist_comp_gans.png?raw=true)
![alt text](https://github.com/StefanoPenazzi2/StefanoPenazzi2.github.io/blob/main/imgs/g_x_linear_opt.png?raw=true)

fun = -1.5290712338367665
x =  1.567, -1.039

![alt text](https://github.com/StefanoPenazzi2/StefanoPenazzi2.github.io/blob/main/imgs/g_x_poli3_opt.png?raw=true)

fun: -3.040389831703985 
x: -1.444e-01,  8.323e-01,  4.859e-01, -2.371e-01

![alt text](https://github.com/StefanoPenazzi2/StefanoPenazzi2.github.io/blob/main/imgs/g_x_deriv_f_x_opt.png?raw=true)


 fun: -1.4346356983844244
 x: 1.016e+00  9.584e-01  2.378e+00  2.332e+00