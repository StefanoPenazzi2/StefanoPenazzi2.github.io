## F-divergence Generative Adversarial Neural Networks

---

### Convex conjugate

**Convex conjugate** _Let $ f : I \to \mathbb{R} $ be a convex function, where $ I \subseteq \mathbb{R} $ is an interval. The convex conjugate of $ f $ is another function $ f^* : I^* \to \mathbb{R} $ defined as:_

$$
f^*(y) = \sup_{x \in \mathbb{R}^n} \left\{ \langle y, x \rangle - f(x) \right\},
$$

_where $ I^* $ is the domain of $ f^* $, determined by the values of $ y $ for which the supremum is finite._ 

![alt text](https://github.com/StefanoPenazzi2/StefanoPenazzi2.github.io/blob/main/imgs/convex_conjugate.png?raw=true)

### F-divergences

![alt text](https://github.com/StefanoPenazzi2/StefanoPenazzi2.github.io/blob/main/imgs/2d_dist_comp_gans.png?raw=true)

The **\( f \)-divergence** is a measure of the difference between two probability distributions P and Q over a measurable space $$ (\Omega, \mathcal{F}) $$. It is defined as:

$$
D_f(P \| Q) = 
\begin{cases} 
\int_\Omega f\left(\frac{dP}{dQ}(x)\right) \, dQ(x), & \text{if } P \ll Q, \\ 
+\infty, & \text{otherwise},
\end{cases}
$$

where:

- $ \frac{dP}{dQ} $ is the **Radonâ€“Nikodym derivative** of P with respect to Q,
- $ P \ll Q $ indicates that P is absolutely continuous with respect to Q (i.e., $ P(A) = 0 $ whenever $ Q(A) = 0 $),
- $ f : I \to \mathbb{R} $ is a convex function satisfying $ f(1) = 0 $.

### F-divergences duality

**Theorem 1** _For any f-divergence, we have:_

$$
D_f(P \| Q) = \sup_{g : \Omega \to \mathbb{R}} \left\{ \mathbb{E}_P[g(X)] - \mathbb{E}_Q[f^*(g(X))] \right\},
$$

where:

- $ P $ and $ Q $ are probability distributions over a common measurable space $ (\Omega, \mathcal{F}) $,
- $ f : I \to \mathbb{R} $ is a convex function,
- $ f^* $ is the convex conjugate of $ f $,
- $ \mathbb{E}_Q[f^{*}(g(X))] $ is the expectation under $ Q $, 
- $ \mathbb{E}_P[g(X)] $ is the expectation of $ g(X) $ under $ P $



**Proof**

$$
\begin{align}
D_f(P \| Q) = \int_\chi  f\left( \frac{dP}{dQ} (x) \right) dQ(x) \\
= \int_\chi \sup_{y \in dom(f^*)} \left( y\frac{dP}{dQ} (x) - f^*(y) \right) dQ(x) \\
\ge \int_\chi \left( g(x)\frac{dP}{dQ} (x) - f^*(g(x)) \right) dQ(x) \\
= \int_\chi g(x)\frac{dP}{dQ} (x) dQ(x) - \int_\chi f^*(g(x)) dQ(x) \\
= \int_\chi g(x) dP(x) - \int_\chi f^*(g(x)) dQ(x) \\
= \mathbb{E}_P[g(X)] - \mathbb{E}_Q[f^*(g(X))]
\end{align}
$$

![alt text](https://github.com/StefanoPenazzi2/StefanoPenazzi2.github.io/blob/main/imgs/g_x_no_opt.png?raw=true)

**Example**

Given two normal distributions (Figure 1) with parameters:

- $ \mu_1 = 1 , \sigma_1 = 1 $ for the first distribution
- $ \mu_2 = 2 , \sigma_2 = 2 $ for the second distribution

![alt text](https://github.com/StefanoPenazzi2/StefanoPenazzi2.github.io/blob/main/imgs/1d_dist_comp_gans.png?raw=true)

we want to calculate the Kullback-Leibler (KL) divergence. As we know, KL divergence is not symmetric, and we have the following results:

- $ D_{KL}(P \| Q) = 0.441796 $
- $ D_{KL}(Q \| P) = 1.279359 $

Now, let's assume we are only able to sample from these two distributions and do not have access to their explicit forms.
This is where the concept of f-divergence duality comes into play.

In this scenario, we sample 10,000 points from both distributions. We use multiple arbitrary functions g(x) to assess the best one.
For each chosen function g(x), we estimate its parameters by maximizing Equation 5 using the Nelder-Mead simplex algorithm. 
This approach allows us to estimate the divergence without needing explicit knowledge of the distributions themselves.

The benchmark is $ D_{KL}(Q \| P) = 1.279359 $, closest the value of the optimization better the result.  

In this first case, we use a linear function $g(x) = a + bx$
![alt text](https://github.com/StefanoPenazzi2/StefanoPenazzi2.github.io/blob/main/imgs/g_x_linear_opt.png?raw=true)

fun = -1.5290712338367665
x =  1.567, -1.039

In this second case, we use a polynomial function $g(x) = a + bx + cx^2 + dx^3$
![alt text](https://github.com/StefanoPenazzi2/StefanoPenazzi2.github.io/blob/main/imgs/g_x_poli3_opt.png?raw=true)

fun: -3.040389831703985 
x: -1.444e-01,  8.323e-01,  4.859e-01, -2.371e-01

In the last case, we use the optimal solution $g(x) = f^'(x)$. This would not be possible since the explicit form of the distributions is unknown.
![alt text](https://github.com/StefanoPenazzi2/StefanoPenazzi2.github.io/blob/main/imgs/g_x_deriv_f_x_opt.png?raw=true)


 fun: -1.4346356983844244
 x: 1.016e+00  9.584e-01  2.378e+00  2.332e+00