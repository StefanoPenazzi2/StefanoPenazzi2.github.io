## f-GAN

---

![alt text](https://github.com/StefanoPenazzi2/StefanoPenazzi2.github.io/blob/main/imgs/fgans/fgans_architecture.png?raw=true)


f-divergence GANs are a class of generative adversarial networks that utilize the concept of f-divergence
to measure the difference between probability distributions. Unlike traditional GANs, which typically use
the Kullback-Leibler or Jensen-Shannon divergences, f-divergence GANs generalize this idea by incorporating
a broader family of divergence measures. These divergences are defined by a convex function $f$, allowing
for a flexible framework to assess the discrepancy between the generated data distribution and the true
data distribution. By adapting the loss functions based on different f-divergences, these GANs can tailor
the learning objectives to suit various applications, potentially improving the quality and stability
of the generated outputs.


<style scoped>
table {
  font-size: 11px;
}
</style>

| Name                    | $D_f(P \| Q)$                                                                                                                                  | Generator $f(u)$                                          |
|-------------------------|------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------|
| Total Variation         | $\frac{1}{2} \int \mid p(x) - q(x) \mid \, dx$                                                                                                 | $\frac{1}{2} \mid u - 1 \mid$                                 |
| Kullback-Leibler        | $\int p(x) \log \frac{p(x)}{q(x)} \, dx$                                                                                                       | $u \log u$                                                |
| Reverse Kullback-Leibler | $\int q(x) \log \frac{q(x)}{p(x)} \, dx$                                                                                                       | $-\log u$                                                 |
| Pearson $\chi^2$      | $\int \frac{(q(x) - p(x))^2}{p(x)} \, dx$                                                                                                      | $(u - 1)^2$                                               |
| Neyman $\chi^2$       | $\int \frac{(p(x) - q(x))^2}{q(x)} \, dx$                                                                                                      | $\frac{(1 - u)^2}{u}$                                     |
| Squared Hellinger       | $\int \left( \sqrt{p(x)} - \sqrt{q(x)} \right)^2 \, dx$                                                                                        | $(\sqrt{u} - 1)^2$                                        |
| Jeffrey                 | $\int (p(x) - q(x)) \log \frac{p(x)}{q(x)} \, dx$                                                                                              | $(u - 1) \log u$                                          |
| Jensen-Shannon          | $\frac{1}{2} \int p(x) \log \frac{2p(x)}{p(x) + q(x)} + q(x) \log \frac{2q(x)}{p(x) + q(x)} \, dx$                                             | $- (u+1) \log \frac{1+u}{2} + u \log u$                   |
| Jensen-Shannon-weighted | $\int \pi p(x) \log \frac{\pi p(x)}{\pi p(x) + (1-\pi) q(x)} + (1-\pi) q(x) \log \frac{(1-\pi) q(x)}{\pi p(x) + (1-\pi) q(x)} \, dx - \log(4)$ | $u \log u - (1 - \pi + \pi u) \log(1 - \pi + \pi u)$      |
| GAN                     | $- \int p(x) \log q(x) + (1 - p(x)) \log(1 - q(x)) \, dx$                                                                                      | $u \log u - (u + 1) \log (u + 1)$                         |
| $\alpha$-divergence   | $\frac{1}{\alpha(\alpha - 1)} \int (p(x)^\alpha q(x)^{1-\alpha} - 1 - \alpha (q(x) - p(x))) \, dx$                                             | $\frac{u^\alpha - 1 - \alpha(u - 1)}{\alpha(\alpha - 1)}$ |

Table above shows a list of f-divergences $D_f(P \| Q)$ along with their generator functions. 
f-divergence family has this form

$$

D_f(P \parallel Q) = \int_{x} q(x) f\left(\frac{p(x)}{q(x)}\right) \, dx,

$$

In the table above $u = \frac{p(x)}{q(x)}$, replacing f\left(\frac{p(x)}{q(x)}\right) in the general f-divergence family
formula with the generator function $f(u)$ we obtain the specific f-divergence.

| Name                    | Conjugate $f^*(t)$                    | Output activation $g_f$            | dom $f^*$            |
|-------------------------|-----------------------------------------|--------------------------------------|------------------------|
| Total Variation         | $t + \frac{1}{2}$                     | $v$                                | $\mathbb{R}$         |
| Kullback-Leibler        | $\exp(t - 1)$                         | $v$                                | $\mathbb{R}$         |
| Reverse Kullback-Leibler| $-1 - \log(-t)$                       | $-\exp(-v)$                        | $\mathbb{R}_-$       |
| Pearson $\chi^2$      | $\frac{t^2}{4} + t$                   | $v$                                | $\mathbb{R}$         |
| Neyman $\chi^2$       | $\frac{t}{1-t}$                       | $1 - \exp(-v)$                     | $t < 1$              |
| Squared Hellinger       | $(\sqrt{t} - 1)^2$                    | $1 - \exp(-v)$                     | $t < 1$              |
| Jeffrey                 | $(t+1) \log(\frac{1+t}{2} + \frac{t}{2})$ | $\log(2) - \log(1 + \exp(-v))$ | $t < \log(2)$        |
| Jensen-Shannon          | $-\log(2 - \exp(t))$                  | $\log(2) - \log(1 + \exp(-v))$     | $t < \log(2)$        |
| Jensen-Shannon-weighted | $- \log(2 - \exp(t))$                 | $\log(2) - \log(1 + \exp(-v))$     | $t < \log(2)$        |
| GAN                     | $- \log(1 - \exp(t))$                 | $- \log(1 + \exp(-v))$             | $\mathbb{R}_-$       |
| $\alpha$-divergence   | $\frac{u^\alpha - 1 - \alpha(u - 1)}{\alpha(\alpha - 1)}$ | variable depending on $\alpha$ | $\mathbb{R}_-$       |

Table above shows the conjugates $ f^*(t) = \sup_{u \in \text{dom}_f} \( ut - f(u) \) $ for each generator function

The associated min-max problem is typically structured as the generator and discriminator trying to optimize opposing objectives

$$
\min_{\theta} \max_{\omega} F(\theta, \omega) = \min_{\theta} \max_{\omega} \left( \mathbb{E}_{x \sim P} \left[g_f(V_\omega(x))\right] + \mathbb{E}_{x \sim Q_\theta} \left[-f^*(g_f(V_\omega(x)))\right]  \right).
$$

- $\min_{\theta}$: The generator minimizes the function to improve its output.
- $\max_{\omega}$: The discriminator maximizes the function to distinguish between real and generated samples effectively. 

**Example: Reverse Kullback-Leibler Divergence**

Consider the function $ f(u) = -\log(u) $, which corresponds to the Reverse Kullback-Leibler divergence. The conjugate $ f^*(t) $ is determined as:

$$ 
f^*(t) = \sup_{u \in \text{dom}_f} ( ut - f(u) ) 
$$

By taking the derivative of $ f^*(t) $ and setting it to zero, we find that:

$$ 
f^*(t) = -1 - \log(-t) 
$$

For this divergence, the min-max problem is structured as follows:

$$ 
\min_{\theta} \max_{\omega} F(\theta, \omega) = \min_{\theta} \max_{\omega} \left( \mathbb{E}_{x \sim P} \left[g_f(V_\omega(x))\right] + \mathbb{E}_{x \sim Q_\theta} \left[\log(-g_f(V_\omega(x)))\right] \right) + 1 
$$

This formulation highlights the interaction between the generator and discriminator within the framework of Reverse Kullback-Leibler divergence.

**Algorithm**

At iteration $t-1$, we begin by finding a suboptimal discriminator $g_f(V_{\omega(t-1)}(x))$ for the current
density output from the generator $Q_{\theta(t-2)}$ which is assumed fixed in this iteration.
We then update the density $Q_{\theta(t-2)} \rightarrow Q_{\theta(t)}$ fixing the updated discriminator
$g_f(V_{\omega(t-1)}(x))$ to enhance accuracy. Repeating this cycle ultimately guides us to the desired solution.

Since $P$ and $Q_\theta$ are unknown throughout the process, but we can sample from these distributions,
the expected value is estimated as follows:

$$

\mathbb{E}_{x \sim P} \left[g_f(V_\omega(x))\right] \approx \frac{1}{|A|} \sum_{x \in A} g_f(V_\omega(x)),

$$

$$

\mathbb{E}_{x \sim Q_\theta} \left[-f^*(g_f(V_\omega(x)))\right] \approx \frac{1}{|B|} \sum_{z \in B} -f^*(g_f(V_\omega(x)))

$$

Set $A$ is a subset of samples drawn from the training/original dataset (a minibatch), set $B$ is a minibatch of samples
in space $R^d$ sampled from $Q_\theta$. 
It's important to note that sampling from $Q_\theta$ can be achieved by remembering that

$$

F_X(x) = P(X \leq x) = P(\theta(Z) \leq x) = P(Z \leq \theta^{-1}(x)) = F_Z(\theta^{-1}(x))

$$

This implies that the same result can be obtained by sampling from $\gamma$
and passing the samples through the generator.
